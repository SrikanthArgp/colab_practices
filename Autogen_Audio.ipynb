{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1zixmvSYj6mNw_l7GDc_ak2tsgSaC3pAQ",
      "authorship_tag": "ABX9TyOpsns+LWtZRTJ2jUz23gbx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SrikanthArgp/colab_practices/blob/main/Autogen_Audio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyMhgnHMCAJl",
        "outputId": "b8781dbc-1efb-4af0-dd66-fe25f027cc19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting autogen-agentchat~=0.2\n",
            "  Downloading autogen_agentchat-0.2.39-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.54.4)\n",
            "Collecting openai-whisper\n",
            "  Downloading openai-whisper-20240930.tar.gz (800 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.5/800.5 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting diskcache (from autogen-agentchat~=0.2)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting docker (from autogen-agentchat~=0.2)\n",
            "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting flaml (from autogen-agentchat~=0.2)\n",
            "  Downloading FLAML-2.3.2-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: numpy<2,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from autogen-agentchat~=0.2) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from autogen-agentchat~=0.2) (24.2)\n",
            "Requirement already satisfied: pydantic!=2.6.0,<3,>=1.10 in /usr/local/lib/python3.10/dist-packages (from autogen-agentchat~=0.2) (2.9.2)\n",
            "Collecting python-dotenv (from autogen-agentchat~=0.2)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from autogen-agentchat~=0.2) (2.5.0)\n",
            "Collecting tiktoken (from autogen-agentchat~=0.2)\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.7.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (0.60.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (2.5.1+cu121)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (10.5.0)\n",
            "Collecting triton>=2.0.0 (from openai-whisper)\n",
            "  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=2.6.0,<3,>=1.10->autogen-agentchat~=0.2) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=2.6.0,<3,>=1.10->autogen-agentchat~=0.2) (2.23.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton>=2.0.0->openai-whisper) (3.16.1)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from docker->autogen-agentchat~=0.2) (2.32.3)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from docker->autogen-agentchat~=0.2) (2.2.3)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->autogen-agentchat~=0.2) (2024.9.11)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->openai-whisper) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->docker->autogen-agentchat~=0.2) (3.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper) (3.0.2)\n",
            "Downloading autogen_agentchat-0.2.39-py3-none-any.whl (382 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m382.2/382.2 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading FLAML-2.3.2-py3-none-any.whl (313 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.9/313.9 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=803320 sha256=10445fad2ca3aba362b243d0aa637a697d0d9fb72a680a275656dd437b9b3927\n",
            "  Stored in directory: /root/.cache/pip/wheels/dd/4a/1f/d1c4bf3b9133c8168fe617ed979cab7b14fe381d059ffb9d83\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: triton, python-dotenv, flaml, diskcache, tiktoken, docker, openai-whisper, autogen-agentchat\n",
            "Successfully installed autogen-agentchat-0.2.39 diskcache-5.6.3 docker-7.1.0 flaml-2.3.2 openai-whisper-20240930 python-dotenv-1.0.1 tiktoken-0.8.0 triton-3.1.0\n"
          ]
        }
      ],
      "source": [
        "pip install autogen-agentchat~=0.2 openai openai-whisper"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "config_list = [\n",
        "    {\n",
        "        \"model\": \"gpt-4\",\n",
        "        \"api_key\": userdata.get('OPENAI_API_KEY'),\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "JnV_hHrNDDZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Annotated, List\n",
        "\n",
        "import whisper\n",
        "from openai import OpenAI\n",
        "\n",
        "import autogen\n",
        "\n",
        "source_language = \"English\"\n",
        "target_language = \"Hindi\"\n",
        "key = userdata.get('OPENAI_API_KEY')\n",
        "target_video = \"/content/drive/MyDrive/video.mp4\"\n",
        "\n",
        "assistant = autogen.AssistantAgent(\n",
        "    name=\"assistant\",\n",
        "    system_message=\"For coding tasks, only use the functions you have been provided with. Reply TERMINATE when the task is done.\",\n",
        "    llm_config={\"config_list\": config_list, \"timeout\": 120},\n",
        ")\n",
        "\n",
        "user_proxy = autogen.UserProxyAgent(\n",
        "    name=\"user_proxy\",\n",
        "    is_termination_msg=lambda x: x.get(\"content\", \"\") and x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
        "    human_input_mode=\"NEVER\",\n",
        "    max_consecutive_auto_reply=10,\n",
        "    code_execution_config={},\n",
        ")\n",
        "\n",
        "\n",
        "def translate_text(input_text, source_language, target_language):\n",
        "    client = OpenAI(api_key=key)\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"Directly translate the following {source_language} text to a pure {target_language} \"\n",
        "                f\"video subtitle text without additional explanation.: '{input_text}'\",\n",
        "            },\n",
        "        ],\n",
        "        max_tokens=1500,\n",
        "    )\n",
        "\n",
        "    # Correctly accessing the response content\n",
        "    translated_text = response.choices[0].message.content if response.choices else None\n",
        "    return translated_text\n",
        "\n",
        "\n",
        "@user_proxy.register_for_execution()\n",
        "@assistant.register_for_llm(description=\"using translate_text function to translate the script\")\n",
        "def translate_transcript(\n",
        "    source_language: Annotated[str, \"Source language\"], target_language: Annotated[str, \"Target language\"]\n",
        ") -> str:\n",
        "    with open(\"transcription.txt\", \"r\") as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    translated_transcript = []\n",
        "    with open(\"translation.txt\", \"w\") as file:\n",
        "      for line in lines:\n",
        "          # Split each line into timestamp and text parts\n",
        "          parts = line.strip().split(\": \")\n",
        "          if len(parts) == 2:\n",
        "              timestamp, text = parts[0], parts[1]\n",
        "              # Translate only the text part\n",
        "              translated_text = translate_text(text, source_language, target_language)\n",
        "              # Reconstruct the line with the translated text and the preserved timestamp\n",
        "              translated_line = f\"{timestamp}: {translated_text}\"\n",
        "              translated_transcript.append(translated_line)\n",
        "              file.write(f\"{translated_line}\\n\")\n",
        "          else:\n",
        "              # If the line doesn't contain a timestamp, add it as is\n",
        "              translated_transcript.append(line.strip())\n",
        "              file.write(f\"{translated_line}\\n\")\n",
        "\n",
        "    return \"\\n\".join(translated_transcript)\n",
        "\n",
        "\n",
        "@user_proxy.register_for_execution()\n",
        "@assistant.register_for_llm(description=\"recognize the speech from video and transfer into a txt file\")\n",
        "def recognize_transcript_from_video(filepath: Annotated[str, \"path of the video file\"]) -> List[dict]:\n",
        "    try:\n",
        "        # Load model\n",
        "        model = whisper.load_model(\"small\")\n",
        "\n",
        "        # Transcribe audio with detailed timestamps\n",
        "        result = model.transcribe(filepath, verbose=True)\n",
        "\n",
        "        # Initialize variables for transcript\n",
        "        transcript = []\n",
        "        sentence = \"\"\n",
        "        start_time = 0\n",
        "\n",
        "        # Iterate through the segments in the result\n",
        "        for segment in result[\"segments\"]:\n",
        "            # If new sentence starts, save the previous one and reset variables\n",
        "            if segment[\"start\"] != start_time and sentence:\n",
        "                transcript.append(\n",
        "                    {\n",
        "                        \"sentence\": sentence.strip() + \".\",\n",
        "                        \"timestamp_start\": start_time,\n",
        "                        \"timestamp_end\": segment[\"start\"],\n",
        "                    }\n",
        "                )\n",
        "                sentence = \"\"\n",
        "                start_time = segment[\"start\"]\n",
        "\n",
        "            # Add the word to the current sentence\n",
        "            sentence += segment[\"text\"] + \" \"\n",
        "\n",
        "        # Add the final sentence\n",
        "        if sentence:\n",
        "            transcript.append(\n",
        "                {\n",
        "                    \"sentence\": sentence.strip() + \".\",\n",
        "                    \"timestamp_start\": start_time,\n",
        "                    \"timestamp_end\": result[\"segments\"][-1][\"end\"],\n",
        "                }\n",
        "            )\n",
        "\n",
        "        # Save the transcript to a file\n",
        "        with open(\"transcription.txt\", \"w\") as file:\n",
        "            for item in transcript:\n",
        "                sentence = item[\"sentence\"]\n",
        "                start_time, end_time = item[\"timestamp_start\"], item[\"timestamp_end\"]\n",
        "                file.write(f\"{start_time}s to {end_time}s: {sentence}\\n\")\n",
        "\n",
        "        return transcript\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        return \"The specified audio file could not be found.\"\n",
        "    except Exception as e:\n",
        "        return f\"An unexpected error occurred: {str(e)}\""
      ],
      "metadata": {
        "id": "plWOcxWFDEPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_proxy.initiate_chat(\n",
        "    assistant,\n",
        "    message=f\"For the video located in {target_video}, recognize the speech and transfer it into a script file, \"\n",
        "    f\"then translate from {source_language} text to a {target_language} video subtitle text. \",\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPUZgatbDH28",
        "outputId": "c835682b-1ba9-44a3-89aa-b8736365cc24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user_proxy (to assistant):\n",
            "\n",
            "For the video located in /content/drive/MyDrive/video.mp4, recognize the speech and transfer it into a script file, then translate from English text to a Hindi video subtitle text. \n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "assistant (to user_proxy):\n",
            "\n",
            "***** Suggested tool call (call_idk9Ctse3GxpA9oEdNWv50yd): recognize_transcript_from_video *****\n",
            "Arguments: \n",
            "{\n",
            "\"filepath\": \"/content/drive/MyDrive/video.mp4\"\n",
            "}\n",
            "************************************************************************************************\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            ">>>>>>>> EXECUTING FUNCTION recognize_transcript_from_video...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(fp, map_location=device)\n",
            "/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detecting language using up to the first 30 seconds. Use `--language` to specify the language\n",
            "Detected language: English\n",
            "[00:00.000 --> 00:06.800]  Lang chain modules. So these are few things that we will try to explore more. We are right here\n",
            "[00:06.800 --> 00:14.080]  in the documentation website of Lang chain and as we can see there are mainly six modules available\n",
            "[00:14.080 --> 00:21.360]  in Lang chain models prompt memory indexes chains agents and callbacks. So we'll go from\n",
            "[00:21.360 --> 00:28.720]  top to bottom and we'll try to dig deeper into all of this modules. So let's go ahead and try to\n",
            "[00:28.720 --> 00:36.240]  understand the first one which is models. As the documentation goes, these are some of the models\n",
            "[00:36.240 --> 00:43.600]  that are being used in Lang chain. So we'll understand the applications of LLMs chart models\n",
            "[00:43.600 --> 00:50.640]  and text embedding models one by one. Moving ahead, I have tried to mention all the modules which\n",
            "[00:50.640 --> 00:54.800]  the Lang chain provides and this topics cover all of them.\n",
            "user_proxy (to assistant):\n",
            "\n",
            "user_proxy (to assistant):\n",
            "\n",
            "***** Response from calling tool (call_idk9Ctse3GxpA9oEdNWv50yd) *****\n",
            "[{\"sentence\": \"Lang chain modules. So these are few things that we will try to explore more. We are right here.\", \"timestamp_start\": 0, \"timestamp_end\": 6.8}, {\"sentence\": \"in the documentation website of Lang chain and as we can see there are mainly six modules available.\", \"timestamp_start\": 6.8, \"timestamp_end\": 14.08}, {\"sentence\": \"in Lang chain models prompt memory indexes chains agents and callbacks. So we'll go from.\", \"timestamp_start\": 14.08, \"timestamp_end\": 21.36}, {\"sentence\": \"top to bottom and we'll try to dig deeper into all of this modules. So let's go ahead and try to.\", \"timestamp_start\": 21.36, \"timestamp_end\": 28.72}, {\"sentence\": \"understand the first one which is models. As the documentation goes, these are some of the models.\", \"timestamp_start\": 28.72, \"timestamp_end\": 36.24}, {\"sentence\": \"that are being used in Lang chain. So we'll understand the applications of LLMs chart models.\", \"timestamp_start\": 36.24, \"timestamp_end\": 43.6}, {\"sentence\": \"and text embedding models one by one. Moving ahead, I have tried to mention all the modules which.\", \"timestamp_start\": 43.6, \"timestamp_end\": 50.64}, {\"sentence\": \"the Lang chain provides and this topics cover all of them..\", \"timestamp_start\": 50.64, \"timestamp_end\": 54.8}]\n",
            "**********************************************************************\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "assistant (to user_proxy):\n",
            "\n",
            "***** Suggested tool call (call_dWhXaJQxxb7Orq5VEsA8MFka): translate_transcript *****\n",
            "Arguments: \n",
            "{\n",
            "\"source_language\": \"en\",\n",
            "\"target_language\": \"hi\"\n",
            "}\n",
            "*************************************************************************************\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            ">>>>>>>> EXECUTING FUNCTION translate_transcript...\n",
            "user_proxy (to assistant):\n",
            "\n",
            "user_proxy (to assistant):\n",
            "\n",
            "***** Response from calling tool (call_dWhXaJQxxb7Orq5VEsA8MFka) *****\n",
            "0s to 6.8s: लैंग श्रृंखला मॉड्यूल। इसलिए ये कुछ चीजें हैं जिन्हें हम अधिक जांचने की कोशिश करेंगे। हम यहाँ सही हैं।\n",
            "6.8s to 14.08s: Lang chain के documentation वेबसाइट में और जैसे हम देख सकते हैं वहाँ मुख्य रूप से छः मॉड्यूल उपलब्ध हैं।\n",
            "14.08s to 21.36s: इस तरह के लैंग श्रृंखला मॉडल स्फीत स्मृति सूचीकरण श्रेणियों एजेंट्स और कॉलबैक को उत्तेजित करते हैं। तो हम आगे बढ़ेंगे।\n",
            "21.36s to 28.72s: 'top से bottom और हम सभी modules में और गहराई से खोजने की कोशिश करेंगे। तो चलिए आगे बढ़ें और कोशिश करें।'\n",
            "28.72s to 36.24s: पहला जिसे मॉडल्स कहा जाता है, उसे समझिए। जैसे ही दस्तावेजीकरण बढ़ता है, ये कुछ मॉडल्स हैं।\n",
            "36.24s to 43.6s: जो Lang श्रृंखला में प्रयोग किया जा रहा है। तो हम LLMs चार्ट मॉडल्स के उपयोग को समझेंगे।\n",
            "43.6s to 50.64s: और पाठ एम्बेडिंग मॉडल्स एक के बाद एक। आगे बढ़ते हुए, मैंने सभी मॉड्यूल्स का उल्लेख करने की कोशिश की है जो।\n",
            "50.64s to 54.8s: लैंग श्रृंखला प्रदान करती है और ये विषय सभी का आवरण करते हैं।\n",
            "**********************************************************************\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "assistant (to user_proxy):\n",
            "\n",
            "TERMINATE\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatResult(chat_id=None, chat_history=[{'content': 'For the video located in /content/drive/MyDrive/video.mp4, recognize the speech and transfer it into a script file, then translate from English text to a Hindi video subtitle text. ', 'role': 'assistant', 'name': 'user_proxy'}, {'tool_calls': [{'id': 'call_idk9Ctse3GxpA9oEdNWv50yd', 'function': {'arguments': '{\\n\"filepath\": \"/content/drive/MyDrive/video.mp4\"\\n}', 'name': 'recognize_transcript_from_video'}, 'type': 'function'}], 'content': None, 'role': 'assistant'}, {'content': '[{\"sentence\": \"Lang chain modules. So these are few things that we will try to explore more. We are right here.\", \"timestamp_start\": 0, \"timestamp_end\": 6.8}, {\"sentence\": \"in the documentation website of Lang chain and as we can see there are mainly six modules available.\", \"timestamp_start\": 6.8, \"timestamp_end\": 14.08}, {\"sentence\": \"in Lang chain models prompt memory indexes chains agents and callbacks. So we\\'ll go from.\", \"timestamp_start\": 14.08, \"timestamp_end\": 21.36}, {\"sentence\": \"top to bottom and we\\'ll try to dig deeper into all of this modules. So let\\'s go ahead and try to.\", \"timestamp_start\": 21.36, \"timestamp_end\": 28.72}, {\"sentence\": \"understand the first one which is models. As the documentation goes, these are some of the models.\", \"timestamp_start\": 28.72, \"timestamp_end\": 36.24}, {\"sentence\": \"that are being used in Lang chain. So we\\'ll understand the applications of LLMs chart models.\", \"timestamp_start\": 36.24, \"timestamp_end\": 43.6}, {\"sentence\": \"and text embedding models one by one. Moving ahead, I have tried to mention all the modules which.\", \"timestamp_start\": 43.6, \"timestamp_end\": 50.64}, {\"sentence\": \"the Lang chain provides and this topics cover all of them..\", \"timestamp_start\": 50.64, \"timestamp_end\": 54.8}]', 'tool_responses': [{'tool_call_id': 'call_idk9Ctse3GxpA9oEdNWv50yd', 'role': 'tool', 'content': '[{\"sentence\": \"Lang chain modules. So these are few things that we will try to explore more. We are right here.\", \"timestamp_start\": 0, \"timestamp_end\": 6.8}, {\"sentence\": \"in the documentation website of Lang chain and as we can see there are mainly six modules available.\", \"timestamp_start\": 6.8, \"timestamp_end\": 14.08}, {\"sentence\": \"in Lang chain models prompt memory indexes chains agents and callbacks. So we\\'ll go from.\", \"timestamp_start\": 14.08, \"timestamp_end\": 21.36}, {\"sentence\": \"top to bottom and we\\'ll try to dig deeper into all of this modules. So let\\'s go ahead and try to.\", \"timestamp_start\": 21.36, \"timestamp_end\": 28.72}, {\"sentence\": \"understand the first one which is models. As the documentation goes, these are some of the models.\", \"timestamp_start\": 28.72, \"timestamp_end\": 36.24}, {\"sentence\": \"that are being used in Lang chain. So we\\'ll understand the applications of LLMs chart models.\", \"timestamp_start\": 36.24, \"timestamp_end\": 43.6}, {\"sentence\": \"and text embedding models one by one. Moving ahead, I have tried to mention all the modules which.\", \"timestamp_start\": 43.6, \"timestamp_end\": 50.64}, {\"sentence\": \"the Lang chain provides and this topics cover all of them..\", \"timestamp_start\": 50.64, \"timestamp_end\": 54.8}]'}], 'role': 'tool', 'name': 'user_proxy'}, {'tool_calls': [{'id': 'call_dWhXaJQxxb7Orq5VEsA8MFka', 'function': {'arguments': '{\\n\"source_language\": \"en\",\\n\"target_language\": \"hi\"\\n}', 'name': 'translate_transcript'}, 'type': 'function'}], 'content': None, 'role': 'assistant'}, {'content': \"0s to 6.8s: लैंग श्रृंखला मॉड्यूल। इसलिए ये कुछ चीजें हैं जिन्हें हम अधिक जांचने की कोशिश करेंगे। हम यहाँ सही हैं।\\n6.8s to 14.08s: Lang chain के documentation वेबसाइट में और जैसे हम देख सकते हैं वहाँ मुख्य रूप से छः मॉड्यूल उपलब्ध हैं।\\n14.08s to 21.36s: इस तरह के लैंग श्रृंखला मॉडल स्फीत स्मृति सूचीकरण श्रेणियों एजेंट्स और कॉलबैक को उत्तेजित करते हैं। तो हम आगे बढ़ेंगे।\\n21.36s to 28.72s: 'top से bottom और हम सभी modules में और गहराई से खोजने की कोशिश करेंगे। तो चलिए आगे बढ़ें और कोशिश करें।'\\n28.72s to 36.24s: पहला जिसे मॉडल्स कहा जाता है, उसे समझिए। जैसे ही दस्तावेजीकरण बढ़ता है, ये कुछ मॉडल्स हैं।\\n36.24s to 43.6s: जो Lang श्रृंखला में प्रयोग किया जा रहा है। तो हम LLMs चार्ट मॉडल्स के उपयोग को समझेंगे।\\n43.6s to 50.64s: और पाठ एम्बेडिंग मॉडल्स एक के बाद एक। आगे बढ़ते हुए, मैंने सभी मॉड्यूल्स का उल्लेख करने की कोशिश की है जो।\\n50.64s to 54.8s: लैंग श्रृंखला प्रदान करती है और ये विषय सभी का आवरण करते हैं।\", 'tool_responses': [{'tool_call_id': 'call_dWhXaJQxxb7Orq5VEsA8MFka', 'role': 'tool', 'content': \"0s to 6.8s: लैंग श्रृंखला मॉड्यूल। इसलिए ये कुछ चीजें हैं जिन्हें हम अधिक जांचने की कोशिश करेंगे। हम यहाँ सही हैं।\\n6.8s to 14.08s: Lang chain के documentation वेबसाइट में और जैसे हम देख सकते हैं वहाँ मुख्य रूप से छः मॉड्यूल उपलब्ध हैं।\\n14.08s to 21.36s: इस तरह के लैंग श्रृंखला मॉडल स्फीत स्मृति सूचीकरण श्रेणियों एजेंट्स और कॉलबैक को उत्तेजित करते हैं। तो हम आगे बढ़ेंगे।\\n21.36s to 28.72s: 'top से bottom और हम सभी modules में और गहराई से खोजने की कोशिश करेंगे। तो चलिए आगे बढ़ें और कोशिश करें।'\\n28.72s to 36.24s: पहला जिसे मॉडल्स कहा जाता है, उसे समझिए। जैसे ही दस्तावेजीकरण बढ़ता है, ये कुछ मॉडल्स हैं।\\n36.24s to 43.6s: जो Lang श्रृंखला में प्रयोग किया जा रहा है। तो हम LLMs चार्ट मॉडल्स के उपयोग को समझेंगे।\\n43.6s to 50.64s: और पाठ एम्बेडिंग मॉडल्स एक के बाद एक। आगे बढ़ते हुए, मैंने सभी मॉड्यूल्स का उल्लेख करने की कोशिश की है जो।\\n50.64s to 54.8s: लैंग श्रृंखला प्रदान करती है और ये विषय सभी का आवरण करते हैं।\"}], 'role': 'tool', 'name': 'user_proxy'}, {'content': 'TERMINATE', 'role': 'user', 'name': 'assistant'}], summary='', cost={'usage_including_cached_inference': {'total_cost': 0.06747, 'gpt-4-0613': {'cost': 0.06747, 'prompt_tokens': 2141, 'completion_tokens': 54, 'total_tokens': 2195}}, 'usage_excluding_cached_inference': {'total_cost': 0.04329, 'gpt-4-0613': {'cost': 0.04329, 'prompt_tokens': 1435, 'completion_tokens': 4, 'total_tokens': 1439}}}, human_input=[])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5QtGjoSWDIy2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}